{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from re import sub\n",
    "import multiprocessing\n",
    "from unidecode import unidecode\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "from time import time \n",
    "from collections import defaultdict\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "file2 = pd.read_csv(\"filtered.csv\")\n",
    "file2 = file[[\"Text\"]]\n",
    "file2['score'] = 1\n",
    "file = file2[1:3000]\n",
    "file_cleaned = file.dropna().drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_word_list(text, remove_polish_letters):\n",
    "    ''' Pre process and convert texts to a list of words \n",
    "    method inspired by method from eliorc github repo: https://github.com/eliorc/Medium/blob/master/MaLSTM.ipynb'''\n",
    "    text = remove_polish_letters(text)\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Clean the text\n",
    "    text = sub(r\"[^A-Za-z0-9^,!?.\\/'+]\", \" \", text)\n",
    "    text = sub(r\"\\+\", \" plus \", text)\n",
    "    text = sub(r\",\", \" \", text)\n",
    "    text = sub(r\"\\.\", \" \", text)\n",
    "    text = sub(r\"!\", \" ! \", text)\n",
    "    text = sub(r\"\\?\", \" ? \", text)\n",
    "    text = sub(r\"'\", \" \", text)\n",
    "    text = sub(r\":\", \" : \", text)\n",
    "    text = sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "    text = text.split()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_cleaned.Text = file_cleaned.Text.apply(lambda x: text_to_word_list(x, unidecode))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_model = file_cleaned.copy()\n",
    "file_model = file_model[file_model.Text.str.len()>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:08:47: collecting all words and their counts\n",
      "INFO - 16:08:47: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 16:08:47: collected 2112 word types from a corpus of 1875 words (unigram + bigrams) and 93 sentences\n",
      "INFO - 16:08:47: using 2112 counts as vocab in Phrases<0 vocab, min_count=1, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 16:08:47: source_vocab length 2112\n",
      "INFO - 16:08:47: Phraser built with 141 phrasegrams\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['great',\n",
       " 'job',\n",
       " 'on',\n",
       " 'the',\n",
       " 'larry',\n",
       " 'king',\n",
       " 'live',\n",
       " 'gulf',\n",
       " 'telethon',\n",
       " 'last',\n",
       " 'night',\n",
       " 'million',\n",
       " 'was',\n",
       " 'raised',\n",
       " 'in',\n",
       " 'hours']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = [row for row in file_model.Text]\n",
    "phrases = Phrases(sent, min_count=1, progress_per=50000)\n",
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[sent]\n",
    "sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:08:47: collecting all words and their counts\n",
      "INFO - 16:08:47: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 16:08:47: collected 752 word types from a corpus of 1590 raw words and 93 sentences\n",
      "INFO - 16:08:47: Loading a fresh vocabulary\n",
      "INFO - 16:08:47: effective_min_count=3 retains 111 unique words (14% of original 752, drops 641)\n",
      "INFO - 16:08:47: effective_min_count=3 leaves 804 word corpus (50% of original 1590, drops 786)\n",
      "INFO - 16:08:47: deleting the raw counts dictionary of 752 items\n",
      "INFO - 16:08:47: sample=1e-05 downsamples 111 most-common words\n",
      "INFO - 16:08:47: downsampling leaves estimated 25 word corpus (3.2% of prior 804)\n",
      "INFO - 16:08:47: estimated required memory for 111 words and 300 dimensions: 321900 bytes\n",
      "INFO - 16:08:47: resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.0 mins\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec(min_count=3,\n",
    "                     window=4,\n",
    "                     size=300,\n",
    "                     sample=1e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=multiprocessing.cpu_count()-1)\n",
    "\n",
    "start = time()\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per=50000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - start) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:08:47: training model with 3 workers on 111 vocabulary and 300 features, using sg=0 hs=0 sample=1e-05 negative=20 window=4\n",
      "INFO - 16:08:47: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:08:47: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:08:47: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:08:47: EPOCH - 1 : training on 1590 raw words (22 effective words) took 0.0s, 2070 effective words/s\n",
      "INFO - 16:08:47: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:08:47: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:08:47: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:08:47: EPOCH - 2 : training on 1590 raw words (21 effective words) took 0.0s, 3752 effective words/s\n",
      "INFO - 16:08:47: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:08:47: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:08:47: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:08:47: EPOCH - 3 : training on 1590 raw words (31 effective words) took 0.0s, 1860 effective words/s\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:08:48: EPOCH - 4 : training on 1590 raw words (26 effective words) took 0.0s, 1364 effective words/s\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:08:48: EPOCH - 5 : training on 1590 raw words (23 effective words) took 0.0s, 1113 effective words/s\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:08:48: EPOCH - 6 : training on 1590 raw words (20 effective words) took 0.0s, 1605 effective words/s\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:08:48: EPOCH - 7 : training on 1590 raw words (22 effective words) took 0.0s, 2727 effective words/s\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:08:48: EPOCH - 8 : training on 1590 raw words (36 effective words) took 0.0s, 3628 effective words/s\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:08:48: EPOCH - 9 : training on 1590 raw words (27 effective words) took 0.0s, 6515 effective words/s\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:08:48: EPOCH - 10 : training on 1590 raw words (25 effective words) took 0.0s, 2296 effective words/s\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:08:48: EPOCH - 11 : training on 1590 raw words (26 effective words) took 0.0s, 1443 effective words/s\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:08:48: EPOCH - 12 : training on 1590 raw words (37 effective words) took 0.0s, 3412 effective words/s\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:08:48: EPOCH - 13 : training on 1590 raw words (22 effective words) took 0.0s, 3885 effective words/s\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:08:48: EPOCH - 14 : training on 1590 raw words (29 effective words) took 0.0s, 1961 effective words/s\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:08:48: EPOCH - 15 : training on 1590 raw words (24 effective words) took 0.0s, 966 effective words/s\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:08:48: EPOCH - 16 : training on 1590 raw words (28 effective words) took 0.0s, 1339 effective words/s\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:08:48: EPOCH - 17 : training on 1590 raw words (26 effective words) took 0.0s, 1703 effective words/s\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:08:48: EPOCH - 18 : training on 1590 raw words (35 effective words) took 0.0s, 1628 effective words/s\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:08:48: EPOCH - 19 : training on 1590 raw words (22 effective words) took 0.0s, 3695 effective words/s\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:08:48: EPOCH - 20 : training on 1590 raw words (30 effective words) took 0.0s, 4194 effective words/s\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:08:48: EPOCH - 21 : training on 1590 raw words (22 effective words) took 0.0s, 5593 effective words/s\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:08:48: EPOCH - 22 : training on 1590 raw words (31 effective words) took 0.0s, 6211 effective words/s\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:08:48: EPOCH - 23 : training on 1590 raw words (28 effective words) took 0.0s, 2960 effective words/s\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:08:48: EPOCH - 24 : training on 1590 raw words (22 effective words) took 0.0s, 2061 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:08:48: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:08:48: EPOCH - 25 : training on 1590 raw words (24 effective words) took 0.0s, 2940 effective words/s\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:08:48: EPOCH - 26 : training on 1590 raw words (31 effective words) took 0.0s, 3281 effective words/s\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:08:48: EPOCH - 27 : training on 1590 raw words (25 effective words) took 0.0s, 1980 effective words/s\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:08:48: EPOCH - 28 : training on 1590 raw words (25 effective words) took 0.0s, 5566 effective words/s\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:08:48: EPOCH - 29 : training on 1590 raw words (30 effective words) took 0.0s, 4497 effective words/s\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:08:48: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:08:48: EPOCH - 30 : training on 1590 raw words (26 effective words) took 0.0s, 3863 effective words/s\n",
      "INFO - 16:08:48: training on a 47700 raw words (796 effective words) took 0.7s, 1092 effective words/s\n",
      "INFO - 16:08:48: precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 0.01 mins\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - start) / 60, 2)))\n",
    "\n",
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:08:48: saving Word2Vec object under word2vec.model, separately None\n",
      "INFO - 16:08:48: not storing attribute vectors_norm\n",
      "INFO - 16:08:48: not storing attribute cum_table\n",
      "INFO - 16:08:48: saved word2vec.model\n"
     ]
    }
   ],
   "source": [
    "w2v_model.save(\"word2vec.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_export = file_model.copy()\n",
    "file_export['old_Text'] = file_export.Text\n",
    "file_export.old_Text = file_export.old_Text.str.join(' ')\n",
    "file_export.Text = file_export.Text.apply(lambda x: ' '.join(bigram[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_export[['Text', 'Date']].to_csv('cleaned_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:08:48: loading Word2Vec object from /Users/pedram/Desktop/Trump-Tweets.nosync/word2vec.model\n",
      "INFO - 16:08:48: loading wv recursively from /Users/pedram/Desktop/Trump-Tweets.nosync/word2vec.model.wv.* with mmap=None\n",
      "INFO - 16:08:48: setting ignored attribute vectors_norm to None\n",
      "INFO - 16:08:48: loading vocabulary recursively from /Users/pedram/Desktop/Trump-Tweets.nosync/word2vec.model.vocabulary.* with mmap=None\n",
      "INFO - 16:08:48: loading trainables recursively from /Users/pedram/Desktop/Trump-Tweets.nosync/word2vec.model.trainables.* with mmap=None\n",
      "INFO - 16:08:48: setting ignored attribute cum_table to None\n",
      "INFO - 16:08:48: loaded /Users/pedram/Desktop/Trump-Tweets.nosync/word2vec.model\n"
     ]
    }
   ],
   "source": [
    "word_vectors = Word2Vec.load(\"/Users/pedram/Desktop/Trump-Tweets.nosync/word2vec.model\").wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=2, max_iter=1000, random_state=True, n_init=50).fit(X=word_vectors.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:08:49: precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('a', 0.2704322040081024),\n",
       " ('on', 0.24872809648513794),\n",
       " ('president', 0.24720978736877441),\n",
       " ('iranian', 0.23861530423164368),\n",
       " ('in', 0.2354060411453247),\n",
       " ('opec_is', 0.23527026176452637),\n",
       " ('energy', 0.235011026263237),\n",
       " ('so', 0.23428875207901),\n",
       " ('off', 0.23241786658763885),\n",
       " ('hotel_collection', 0.23020373284816742)]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_vector(model.cluster_centers_[0], topn=10, restrict_vocab=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_cluster_center = model.cluster_centers_[0]\n",
    "negative_cluster_center = model.cluster_centers_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "words = pd.DataFrame(word_vectors.vocab.keys())\n",
    "words.columns = ['words']\n",
    "words['vectors'] = words.words.apply(lambda x: word_vectors.wv[f'{x}'])\n",
    "words['cluster'] = words.vectors.apply(lambda x: model.predict([np.array(x)]))\n",
    "words.cluster = words.cluster.apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "words['cluster_value'] = [1 if i==0 else -1 for i in words.cluster]\n",
    "words['closeness_score'] = words.apply(lambda x: 1/(model.transform([x.vectors]).min()), axis=1)\n",
    "words['sentiment_coeff'] = words.closeness_score * words.cluster_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>vectors</th>\n",
       "      <th>cluster</th>\n",
       "      <th>cluster_value</th>\n",
       "      <th>closeness_score</th>\n",
       "      <th>sentiment_coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>looking_forward</td>\n",
       "      <td>[-0.08035743, 0.004391101, 0.094596125, -0.039...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.016541</td>\n",
       "      <td>-1.016541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>to</td>\n",
       "      <td>[-0.055363644, -0.021763844, -0.09545412, -0.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.004344</td>\n",
       "      <td>-1.004344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>the_miss</td>\n",
       "      <td>[-0.055421975, 0.08603286, 0.020669483, -0.050...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.999853</td>\n",
       "      <td>-0.999853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>on_nbc</td>\n",
       "      <td>[0.07019112, -0.027717268, 0.024311414, 0.0727...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.018604</td>\n",
       "      <td>1.018604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>p_m</td>\n",
       "      <td>[-0.032562632, -0.045807403, -0.09336083, -0.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.023643</td>\n",
       "      <td>-1.023643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>by</td>\n",
       "      <td>[-0.034352284, -0.062403064, 0.051247723, -0.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.013388</td>\n",
       "      <td>-1.013388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>las_vegas</td>\n",
       "      <td>[0.077530466, -0.061023492, 0.033851057, -0.02...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.009837</td>\n",
       "      <td>-1.009837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>on</td>\n",
       "      <td>[0.0051376405, 0.09358468, 0.05252527, 0.08261...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.029869</td>\n",
       "      <td>1.029869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>the</td>\n",
       "      <td>[0.027780207, 0.092891015, 0.07208228, 0.04041...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.020740</td>\n",
       "      <td>1.020740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>raised</td>\n",
       "      <td>[-0.005164237, -0.014353212, -0.0011276278, 0....</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.001386</td>\n",
       "      <td>-1.001386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             words                                            vectors  \\\n",
       "0  looking_forward  [-0.08035743, 0.004391101, 0.094596125, -0.039...   \n",
       "1               to  [-0.055363644, -0.021763844, -0.09545412, -0.0...   \n",
       "2         the_miss  [-0.055421975, 0.08603286, 0.020669483, -0.050...   \n",
       "3           on_nbc  [0.07019112, -0.027717268, 0.024311414, 0.0727...   \n",
       "4              p_m  [-0.032562632, -0.045807403, -0.09336083, -0.0...   \n",
       "5               by  [-0.034352284, -0.062403064, 0.051247723, -0.0...   \n",
       "6        las_vegas  [0.077530466, -0.061023492, 0.033851057, -0.02...   \n",
       "7               on  [0.0051376405, 0.09358468, 0.05252527, 0.08261...   \n",
       "8              the  [0.027780207, 0.092891015, 0.07208228, 0.04041...   \n",
       "9           raised  [-0.005164237, -0.014353212, -0.0011276278, 0....   \n",
       "\n",
       "   cluster  cluster_value  closeness_score  sentiment_coeff  \n",
       "0        1             -1         1.016541        -1.016541  \n",
       "1        1             -1         1.004344        -1.004344  \n",
       "2        1             -1         0.999853        -0.999853  \n",
       "3        0              1         1.018604         1.018604  \n",
       "4        1             -1         1.023643        -1.023643  \n",
       "5        1             -1         1.013388        -1.013388  \n",
       "6        1             -1         1.009837        -1.009837  \n",
       "7        0              1         1.029869         1.029869  \n",
       "8        0              1         1.020740         1.020740  \n",
       "9        1             -1         1.001386        -1.001386  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[['words', 'sentiment_coeff']].to_csv('sentiment_dictionary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_file = pd.read_csv('/Users/pedram/Desktop/Trump-Tweets.nosync/cleaned_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_map = pd.read_csv('/Users/pedram/Desktop/Trump-Tweets.nosync/sentiment_dictionary.csv')\n",
    "sentiment_dict = dict(zip(sentiment_map.words.values, sentiment_map.sentiment_coeff.values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_weighting = final_file.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=lambda y: y.split(), norm=None)\n",
    "tfidf.fit(file_weighting.Text)\n",
    "features = pd.Series(tfidf.get_feature_names())\n",
    "transformed = tfidf.transform(file_weighting.Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfidf_dictionary(x, transformed_file, features):\n",
    "    '''\n",
    "    create dictionary for each input sentence x, where each word has assigned its tfidf score\n",
    "    \n",
    "    inspired  by function from this wonderful article: \n",
    "    https://medium.com/analytics-vidhya/automated-keyword-extraction-from-articles-using-nlp-bfd864f41b34\n",
    "    \n",
    "    x - row of dataframe, containing sentences, and their indexes,\n",
    "    transformed_file - all sentences transformed with TfidfVectorizer\n",
    "    features - names of all words in corpus used in TfidfVectorizer\n",
    "\n",
    "    '''\n",
    "    vector_coo = transformed_file[x.name].tocoo()\n",
    "    vector_coo.col = features.iloc[vector_coo.col].values\n",
    "    dict_from_coo = dict(zip(vector_coo.col, vector_coo.data))\n",
    "    return dict_from_coo\n",
    "\n",
    "def replace_tfidf_words(x, transformed_file, features):\n",
    "    '''\n",
    "    replacing each word with it's calculated tfidf dictionary with scores of each word\n",
    "    x - row of dataframe, containing sentences, and their indexes,\n",
    "    transformed_file - all sentences transformed with TfidfVectorizer\n",
    "    features - names of all words in corpus used in TfidfVectorizer\n",
    "    '''\n",
    "    dictionary = create_tfidf_dictionary(x, transformed_file, features)   \n",
    "    return list(map(lambda y:dictionary[f'{y}'], x.Text.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 77.3 ms, sys: 5.76 ms, total: 83.1 ms\n",
      "Wall time: 84.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "replaced_tfidf_scores = file_weighting.apply(lambda x: replace_tfidf_words(x, transformed, features), axis=1)#this step takes around 3-4 minutes minutes to calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def replace_sentiment_words(word, sentiment_dict):\n",
    "    '''\n",
    "    replacing each word with its associated sentiment score from sentiment dict\n",
    "    '''\n",
    "    try:\n",
    "        out = sentiment_dict[word]\n",
    "    except KeyError:\n",
    "        out = 0\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "replaced_closeness_scores = file_weighting.Text.apply(lambda x: list(map(lambda y: replace_sentiment_words(y, sentiment_dict), x.split())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_df = pd.DataFrame(data=[replaced_closeness_scores, replaced_tfidf_scores, file_weighting.Text, file_weighting.score]).T\n",
    "replacement_df.columns = ['sentiment_coeff', 'tfidf_scores', 'sentence', 'sentiment']\n",
    "replacement_df['sentiment_rate'] = replacement_df.apply(lambda x: np.array(x.loc['sentiment_coeff']) @ np.array(x.loc['tfidf_scores']), axis=1)\n",
    "replacement_df['prediction'] = (replacement_df.sentiment_rate>0).astype('int8')\n",
    "replacement_df['sentiment'] = [1 if i==1 else 0 for i in replacement_df.sentiment]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0  65  28\n",
       "1   0   0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " Scores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.698925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>precision</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>recall</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>f1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             scores\n",
       "accuracy   0.698925\n",
       "precision  0.000000\n",
       "recall     0.000000\n",
       "f1         0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted_classes = replacement_df.prediction\n",
    "y_test = replacement_df.sentiment\n",
    "\n",
    "conf_matrix = pd.DataFrame(confusion_matrix(replacement_df.sentiment, replacement_df.prediction))\n",
    "print('Confusion Matrix')\n",
    "display(conf_matrix)\n",
    "\n",
    "test_scores = accuracy_score(y_test,predicted_classes), precision_score(y_test, predicted_classes), recall_score(y_test, predicted_classes), f1_score(y_test, predicted_classes)\n",
    "\n",
    "print('\\n \\n Scores')\n",
    "scores = pd.DataFrame(data=[test_scores])\n",
    "scores.columns = ['accuracy', 'precision', 'recall', 'f1']\n",
    "scores = scores.T\n",
    "scores.columns = ['scores']\n",
    "display(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
