{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from re import sub\n",
    "import multiprocessing\n",
    "from unidecode import unidecode\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "from time import time \n",
    "from collections import defaultdict\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file2 = pd.read_csv(\"evaluationdata.csv\")\n",
    "file2 = file2[[\"Text\",\"bing_score\"]]\n",
    "file2.columns= ['title', 'rate']\n",
    "file_cleaned = file2.dropna().drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:18:20: NumExpr defaulting to 4 threads.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       " 0    0.353409\n",
       "-1    0.329370\n",
       " 1    0.317221\n",
       "Name: rate, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_cleaned.rate.value_counts()/len(file_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>so to all americans in every city near and far...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>the forgotten men and women of our country wil...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>what truly matters is not which party controls...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>we will bring back our jobs we will bring back...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>it is time to remember that  t co zkyoioor</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  rate\n",
       "0  so to all americans in every city near and far...     0\n",
       "1  the forgotten men and women of our country wil...     0\n",
       "2  what truly matters is not which party controls...     0\n",
       "3  we will bring back our jobs we will bring back...     0\n",
       "4        it is time to remember that  t co zkyoioor      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_cleaned[file_cleaned.rate==0]\n",
    "file_cleaned = file_cleaned[file_cleaned.rate!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_cleaned.rate.value_counts()/len(file_cleaned)\n",
    "file_model = file_cleaned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:18:30: collecting all words and their counts\n",
      "INFO - 22:18:30: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 22:18:35: collected 756 word types from a corpus of 1702142 words (unigram + bigrams) and 10006 sentences\n",
      "INFO - 22:18:35: using 756 counts as vocab in Phrases<0 vocab, min_count=1, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 22:18:35: source_vocab length 756\n",
      "INFO - 22:18:35: Phraser built with 0 phrasegrams\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a fantastic day and evening in washington d c thank you to foxnews and so many other news outlets for the great reviews of the speech '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = [row for row in file_cleaned.title]\n",
    "phrases = Phrases(sent, min_count=1, progress_per=50000)\n",
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[sent]\n",
    "sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:18:35: collecting all words and their counts\n",
      "WARNING - 22:18:35: Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
      "INFO - 22:18:35: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 22:18:35: collected 27 word types from a corpus of 1702142 raw words and 10006 sentences\n",
      "INFO - 22:18:35: Loading a fresh vocabulary\n",
      "INFO - 22:18:35: effective_min_count=3 retains 27 unique words (100% of original 27, drops 0)\n",
      "INFO - 22:18:35: effective_min_count=3 leaves 1702142 word corpus (100% of original 1702142, drops 0)\n",
      "INFO - 22:18:35: deleting the raw counts dictionary of 27 items\n",
      "INFO - 22:18:35: sample=1e-05 downsamples 27 most-common words\n",
      "INFO - 22:18:35: downsampling leaves estimated 25201 word corpus (1.5% of prior 1702142)\n",
      "INFO - 22:18:35: estimated required memory for 27 words and 300 dimensions: 78300 bytes\n",
      "INFO - 22:18:35: resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.01 mins\n"
     ]
    }
   ],
   "source": [
    "\n",
    "w2v_model = Word2Vec(min_count=3,\n",
    "                     window=4,\n",
    "                     size=300,\n",
    "                     sample=1e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=multiprocessing.cpu_count()-1)\n",
    "\n",
    "start = time()\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per=50000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - start) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:18:35: training model with 3 workers on 27 vocabulary and 300 features, using sg=0 hs=0 sample=1e-05 negative=20 window=4\n",
      "INFO - 22:18:36: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:18:36: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:18:36: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:18:36: EPOCH - 1 : training on 1702142 raw words (25141 effective words) took 0.3s, 75332 effective words/s\n",
      "INFO - 22:18:36: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:18:36: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:18:36: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:18:36: EPOCH - 2 : training on 1702142 raw words (25230 effective words) took 0.3s, 76756 effective words/s\n",
      "INFO - 22:18:36: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:18:36: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:18:36: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:18:36: EPOCH - 3 : training on 1702142 raw words (25145 effective words) took 0.3s, 72311 effective words/s\n",
      "INFO - 22:18:37: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:18:37: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:18:37: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:18:37: EPOCH - 4 : training on 1702142 raw words (24996 effective words) took 0.4s, 58633 effective words/s\n",
      "INFO - 22:18:37: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:18:37: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:18:37: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:18:37: EPOCH - 5 : training on 1702142 raw words (25077 effective words) took 0.4s, 67095 effective words/s\n",
      "INFO - 22:18:37: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:18:37: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:18:37: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:18:37: EPOCH - 6 : training on 1702142 raw words (25154 effective words) took 0.3s, 75990 effective words/s\n",
      "INFO - 22:18:38: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:18:38: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:18:38: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:18:38: EPOCH - 7 : training on 1702142 raw words (25233 effective words) took 0.4s, 71433 effective words/s\n",
      "INFO - 22:18:38: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:18:38: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:18:38: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:18:38: EPOCH - 8 : training on 1702142 raw words (25115 effective words) took 0.3s, 73133 effective words/s\n",
      "INFO - 22:18:38: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:18:38: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:18:38: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:18:38: EPOCH - 9 : training on 1702142 raw words (25147 effective words) took 0.3s, 73691 effective words/s\n",
      "INFO - 22:18:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:18:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:18:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:18:39: EPOCH - 10 : training on 1702142 raw words (25062 effective words) took 0.3s, 84304 effective words/s\n",
      "INFO - 22:18:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:18:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:18:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:18:39: EPOCH - 11 : training on 1702142 raw words (25152 effective words) took 0.3s, 72219 effective words/s\n",
      "INFO - 22:18:40: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:18:40: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:18:40: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:18:40: EPOCH - 12 : training on 1702142 raw words (25213 effective words) took 0.3s, 72384 effective words/s\n",
      "INFO - 22:18:40: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:18:40: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:18:40: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:18:40: EPOCH - 13 : training on 1702142 raw words (25386 effective words) took 0.4s, 59179 effective words/s\n",
      "INFO - 22:18:40: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:18:40: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:18:40: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:18:40: EPOCH - 14 : training on 1702142 raw words (25363 effective words) took 0.3s, 80247 effective words/s\n",
      "INFO - 22:18:41: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:18:41: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:18:41: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:18:41: EPOCH - 15 : training on 1702142 raw words (25217 effective words) took 0.3s, 84060 effective words/s\n",
      "INFO - 22:18:41: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:18:41: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:18:41: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:18:41: EPOCH - 16 : training on 1702142 raw words (25412 effective words) took 0.3s, 79771 effective words/s\n",
      "INFO - 22:18:41: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:18:41: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:18:41: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:18:41: EPOCH - 17 : training on 1702142 raw words (25116 effective words) took 0.3s, 78785 effective words/s\n",
      "INFO - 22:18:42: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:18:42: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:18:42: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:18:42: EPOCH - 18 : training on 1702142 raw words (25435 effective words) took 0.3s, 75704 effective words/s\n",
      "INFO - 22:18:42: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:18:42: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:18:42: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:18:42: EPOCH - 19 : training on 1702142 raw words (25044 effective words) took 0.3s, 80967 effective words/s\n",
      "INFO - 22:18:42: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:18:42: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:18:42: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:18:42: EPOCH - 20 : training on 1702142 raw words (25034 effective words) took 0.3s, 78281 effective words/s\n",
      "INFO - 22:18:43: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:18:43: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:18:43: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:18:43: EPOCH - 21 : training on 1702142 raw words (25193 effective words) took 0.4s, 67367 effective words/s\n",
      "INFO - 22:18:43: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:18:43: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:18:43: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:18:43: EPOCH - 22 : training on 1702142 raw words (25043 effective words) took 0.3s, 83016 effective words/s\n",
      "INFO - 22:18:43: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:18:43: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:18:43: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:18:43: EPOCH - 23 : training on 1702142 raw words (25444 effective words) took 0.3s, 78575 effective words/s\n",
      "INFO - 22:18:44: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:18:44: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:18:44: worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:18:44: EPOCH - 24 : training on 1702142 raw words (25337 effective words) took 0.3s, 76889 effective words/s\n",
      "INFO - 22:18:44: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:18:44: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:18:44: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:18:44: EPOCH - 25 : training on 1702142 raw words (25198 effective words) took 0.3s, 79765 effective words/s\n",
      "INFO - 22:18:44: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:18:44: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:18:44: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:18:44: EPOCH - 26 : training on 1702142 raw words (25291 effective words) took 0.4s, 71506 effective words/s\n",
      "INFO - 22:18:45: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:18:45: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:18:45: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:18:45: EPOCH - 27 : training on 1702142 raw words (25203 effective words) took 0.3s, 75464 effective words/s\n",
      "INFO - 22:18:45: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:18:45: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:18:45: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:18:45: EPOCH - 28 : training on 1702142 raw words (25337 effective words) took 0.3s, 86371 effective words/s\n",
      "INFO - 22:18:45: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:18:45: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:18:45: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:18:45: EPOCH - 29 : training on 1702142 raw words (25427 effective words) took 0.3s, 85338 effective words/s\n",
      "INFO - 22:18:46: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:18:46: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:18:46: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:18:46: EPOCH - 30 : training on 1702142 raw words (25238 effective words) took 0.3s, 83335 effective words/s\n",
      "INFO - 22:18:46: training on a 51064260 raw words (756383 effective words) took 10.4s, 73063 effective words/s\n",
      "INFO - 22:18:46: precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 0.17 mins\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - start) / 60, 2)))\n",
    "\n",
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:18:46: saving Word2Vec object under word2vec.model, separately None\n",
      "INFO - 22:18:46: not storing attribute vectors_norm\n",
      "INFO - 22:18:46: not storing attribute cum_table\n",
      "INFO - 22:18:46: saved word2vec.model\n"
     ]
    }
   ],
   "source": [
    "w2v_model.save(\"word2vec.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_export = file_model.copy()\n",
    "file_export['old_title'] = file_export.title\n",
    "file_export.old_title = file_export.old_title.str.join(' ')\n",
    "file_export.title = file_export.title.apply(lambda x: ' '.join(bigram[x]))\n",
    "file_export.rate = file_export.rate.astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_export[['title', 'rate']].to_csv('cleaned_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:18:53: loading Word2Vec object from /Users/pedram/Desktop/Trump-Tweets.nosync/word2vec.model\n",
      "INFO - 22:18:53: loading wv recursively from /Users/pedram/Desktop/Trump-Tweets.nosync/word2vec.model.wv.* with mmap=None\n",
      "INFO - 22:18:53: setting ignored attribute vectors_norm to None\n",
      "INFO - 22:18:53: loading vocabulary recursively from /Users/pedram/Desktop/Trump-Tweets.nosync/word2vec.model.vocabulary.* with mmap=None\n",
      "INFO - 22:18:53: loading trainables recursively from /Users/pedram/Desktop/Trump-Tweets.nosync/word2vec.model.trainables.* with mmap=None\n",
      "INFO - 22:18:53: setting ignored attribute cum_table to None\n",
      "INFO - 22:18:53: loaded /Users/pedram/Desktop/Trump-Tweets.nosync/word2vec.model\n"
     ]
    }
   ],
   "source": [
    "word_vectors = Word2Vec.load(\"/Users/pedram/Desktop/Trump-Tweets.nosync/word2vec.model\").wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=3, max_iter=1000, random_state=True, n_init=50).fit(X=word_vectors.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:18:54: precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('o', 0.9942623376846313),\n",
       " ('a', 0.973189115524292),\n",
       " ('s', 0.9616868495941162),\n",
       " ('w', 0.9593265652656555),\n",
       " (' ', 0.950028657913208)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_vector(model.cluster_centers_[0], topn=5, restrict_vocab=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_cluster_center = model.cluster_centers_[2]\n",
    "negative_cluster_center = model.cluster_centers_[0]\n",
    "neutral_cluster_center = model.cluster_centers_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "words = pd.DataFrame(word_vectors.vocab.keys())\n",
    "words.columns = ['words']\n",
    "words['vectors'] = words.words.apply(lambda x: word_vectors.wv[f'{x}'])\n",
    "words['cluster'] = words.vectors.apply(lambda x: model.predict([np.array(x)]))\n",
    "words.cluster = words.cluster.apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "words['cluster_value'] = [-1 if i==0 else 0 if i==1 else 1 for i in words.cluster]\n",
    "words['closeness_score'] = words.apply(lambda x: 1/(model.transform([x.vectors]).min()), axis=1)\n",
    "words['sentiment_coeff'] = words.closeness_score * words.cluster_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>vectors</th>\n",
       "      <th>cluster</th>\n",
       "      <th>cluster_value</th>\n",
       "      <th>closeness_score</th>\n",
       "      <th>sentiment_coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>[0.07202761, -0.0023901272, 0.021249838, -0.05...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.714276</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>h</td>\n",
       "      <td>[0.14253461, 0.019613042, 0.042470526, -0.0956...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2.830169</td>\n",
       "      <td>-2.830169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>a</td>\n",
       "      <td>[0.13781895, 0.0033878789, 0.047259863, -0.093...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>4.332484</td>\n",
       "      <td>-4.332484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>n</td>\n",
       "      <td>[0.1518605, 0.016812986, 0.058567338, -0.11737...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2.990758</td>\n",
       "      <td>-2.990758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>k</td>\n",
       "      <td>[0.039985213, -0.049300868, 0.018029511, -0.04...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6.452795</td>\n",
       "      <td>6.452795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td>[0.12264263, 0.03648119, 0.04588394, -0.086476...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>3.203212</td>\n",
       "      <td>-3.203212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>y</td>\n",
       "      <td>[0.115900256, -0.015490524, 0.02904452, -0.099...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.662603</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>o</td>\n",
       "      <td>[0.14557023, 0.0043658884, 0.044960614, -0.106...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>8.746305</td>\n",
       "      <td>-8.746305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>u</td>\n",
       "      <td>[0.09511685, 0.028388238, 0.035173774, -0.0836...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.717623</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>f</td>\n",
       "      <td>[0.08910028, -0.047725752, 0.02075924, -0.0616...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.798188</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  words                                            vectors  cluster  \\\n",
       "0     t  [0.07202761, -0.0023901272, 0.021249838, -0.05...        1   \n",
       "1     h  [0.14253461, 0.019613042, 0.042470526, -0.0956...        0   \n",
       "2     a  [0.13781895, 0.0033878789, 0.047259863, -0.093...        0   \n",
       "3     n  [0.1518605, 0.016812986, 0.058567338, -0.11737...        0   \n",
       "4     k  [0.039985213, -0.049300868, 0.018029511, -0.04...        2   \n",
       "5        [0.12264263, 0.03648119, 0.04588394, -0.086476...        0   \n",
       "6     y  [0.115900256, -0.015490524, 0.02904452, -0.099...        1   \n",
       "7     o  [0.14557023, 0.0043658884, 0.044960614, -0.106...        0   \n",
       "8     u  [0.09511685, 0.028388238, 0.035173774, -0.0836...        1   \n",
       "9     f  [0.08910028, -0.047725752, 0.02075924, -0.0616...        1   \n",
       "\n",
       "   cluster_value  closeness_score  sentiment_coeff  \n",
       "0              0         2.714276         0.000000  \n",
       "1             -1         2.830169        -2.830169  \n",
       "2             -1         4.332484        -4.332484  \n",
       "3             -1         2.990758        -2.990758  \n",
       "4              1         6.452795         6.452795  \n",
       "5             -1         3.203212        -3.203212  \n",
       "6              0         7.662603         0.000000  \n",
       "7             -1         8.746305        -8.746305  \n",
       "8              0         2.717623         0.000000  \n",
       "9              0         3.798188         0.000000  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[['words', 'sentiment_coeff']].to_csv('sentiment_dictionary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_file = pd.read_csv('/Users/pedram/Desktop/Trump-Tweets.nosync/cleaned_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_map = pd.read_csv('/Users/pedram/Desktop/Trump-Tweets.nosync/sentiment_dictionary.csv')\n",
    "sentiment_dict = dict(zip(sentiment_map.words.values, sentiment_map.sentiment_coeff.values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_weighting = final_file.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=lambda y: y.split(), norm=None)\n",
    "tfidf.fit(file_weighting.title)\n",
    "features = pd.Series(tfidf.get_feature_names())\n",
    "transformed = tfidf.transform(file_weighting.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfidf_dictionary(x, transformed_file, features):\n",
    "    '''\n",
    "    create dictionary for each input sentence x, where each word has assigned its tfidf score\n",
    "    \n",
    "    inspired  by function from this wonderful article: \n",
    "    https://medium.com/analytics-vidhya/automated-keyword-extraction-from-articles-using-nlp-bfd864f41b34\n",
    "    \n",
    "    x - row of dataframe, containing sentences, and their indexes,\n",
    "    transformed_file - all sentences transformed with TfidfVectorizer\n",
    "    features - names of all words in corpus used in TfidfVectorizer\n",
    "\n",
    "    '''\n",
    "    vector_coo = transformed_file[x.name].tocoo()\n",
    "    vector_coo.col = features.iloc[vector_coo.col].values\n",
    "    dict_from_coo = dict(zip(vector_coo.col, vector_coo.data))\n",
    "    return dict_from_coo\n",
    "\n",
    "def replace_tfidf_words(x, transformed_file, features):\n",
    "    '''\n",
    "    replacing each word with it's calculated tfidf dictionary with scores of each word\n",
    "    x - row of dataframe, containing sentences, and their indexes,\n",
    "    transformed_file - all sentences transformed with TfidfVectorizer\n",
    "    features - names of all words in corpus used in TfidfVectorizer\n",
    "    '''\n",
    "    dictionary = create_tfidf_dictionary(x, transformed_file, features)   \n",
    "    return list(map(lambda y:dictionary[f'{y}'], x.title.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.12 s, sys: 58 ms, total: 5.18 s\n",
      "Wall time: 5.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "replaced_tfidf_scores = file_weighting.apply(lambda x: replace_tfidf_words(x, transformed, features), axis=1)#this step takes around 3-4 minutes minutes to calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_sentiment_words(word, sentiment_dict):\n",
    "    '''\n",
    "    replacing each word with its associated sentiment score from sentiment dict\n",
    "    '''\n",
    "    try:\n",
    "        out = sentiment_dict[word]\n",
    "    except KeyError:\n",
    "        out = 0\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "replaced_closeness_scores = file_weighting.title.apply(lambda x: list(map(lambda y: replace_sentiment_words(y, sentiment_dict), x.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_df = pd.DataFrame(data=[replaced_closeness_scores, replaced_tfidf_scores, file_weighting.title, file_weighting.rate]).T\n",
    "replacement_df.columns = ['sentiment_coeff', 'tfidf_scores', 'sentence', 'sentiment']\n",
    "replacement_df['sentiment_rate'] = replacement_df.apply(lambda x: np.array(x.loc['sentiment_coeff']) @ np.array(x.loc['tfidf_scores']), axis=1)\n",
    "replacement_df['prediction'] = [-1 if i<-7000 else 1 if i> -1000 else 0 for i in replacement_df.sentiment_rate]\n",
    "replacement_df['sentiment'] = [-1 if i==0 else 0 if i==1 else 1 for i in replacement_df.sentiment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_df.to_csv(r'/Users/pedram/Desktop/Trump-Tweets.nosync/finalizing_nrc.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>960</td>\n",
       "      <td>3098</td>\n",
       "      <td>851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1575</td>\n",
       "      <td>3063</td>\n",
       "      <td>459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1    2\n",
       "0     0     0    0\n",
       "1   960  3098  851\n",
       "2  1575  3063  459"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-dcb427d36e89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtest_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredicted_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n \\n Scores'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mprecision_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[1;32m   1567\u001b[0m                                                  \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'precision'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1569\u001b[0;31m                                                  sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1570\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)\u001b[0m\n\u001b[1;32m   1413\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m     labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n\u001b[0;32m-> 1415\u001b[0;31m                                     pos_label)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1252\u001b[0m             raise ValueError(\"Target is %s but average='binary'. Please \"\n\u001b[1;32m   1253\u001b[0m                              \u001b[0;34m\"choose another average setting, one of %r.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m                              % (y_type, average_options))\n\u001b[0m\u001b[1;32m   1255\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mpos_label\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         warnings.warn(\"Note that pos_label (set to %r) is ignored when \"\n",
      "\u001b[0;31mValueError\u001b[0m: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted']."
     ]
    }
   ],
   "source": [
    "predicted_classes = replacement_df.prediction\n",
    "y_test = replacement_df.sentiment\n",
    "\n",
    "conf_matrix = pd.DataFrame(confusion_matrix(replacement_df.sentiment, replacement_df.prediction))\n",
    "print('Confusion Matrix')\n",
    "display(conf_matrix)\n",
    "\n",
    "test_scores = accuracy_score(y_test,predicted_classes), precision_score(y_test, predicted_classes), recall_score(y_test, predicted_classes), f1_score(y_test, predicted_classes)\n",
    "\n",
    "print('\\n \\n Scores')\n",
    "scores = pd.DataFrame(data=[test_scores])\n",
    "scores.columns = ['accuracy', 'precision', 'recall', 'f1']\n",
    "scores = scores.T\n",
    "scores.columns = ['scores']\n",
    "display(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
