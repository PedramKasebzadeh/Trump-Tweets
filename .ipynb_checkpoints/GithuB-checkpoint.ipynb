{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from re import sub\n",
    "import multiprocessing\n",
    "from unidecode import unidecode\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "from time import time \n",
    "from collections import defaultdict\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file2 = pd.read_csv(\"filtered.csv\")\n",
    "file2 = file2[[\"Text\"]]\n",
    "file2['rate'] = 1\n",
    "file2.columns= ['title', 'rate']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>be aware of things that seem inexplicable bec...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>the trump hotel collection is currently nomina...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>don t forget the open call at trump tower tomo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>looking forward to seeing the world champion y...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>to put on your calendar for may miss usa live ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  rate\n",
       "0   be aware of things that seem inexplicable bec...     2\n",
       "1  the trump hotel collection is currently nomina...     3\n",
       "2  don t forget the open call at trump tower tomo...     1\n",
       "3  looking forward to seeing the world champion y...     1\n",
       "4  to put on your calendar for may miss usa live ...    -1"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file2 = pd.read_csv(\"filtered.csv\")\n",
    "file2 = file2[[\"Text\",\"bing_score\"]]\n",
    "file2.columns= ['title', 'rate']\n",
    "file = file2[1:3000]\n",
    "file_cleaned = file.dropna().drop_duplicates().reset_index(drop=True)\n",
    "file_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0    0.330582\n",
       "-1    0.188097\n",
       " 1    0.175934\n",
       " 2    0.086881\n",
       "-2    0.084275\n",
       "-3    0.046047\n",
       " 3    0.043440\n",
       "-4    0.016942\n",
       " 4    0.011729\n",
       "-5    0.004778\n",
       " 5    0.004344\n",
       "-6    0.003041\n",
       " 6    0.002172\n",
       "-7    0.000869\n",
       " 7    0.000434\n",
       "-8    0.000434\n",
       "Name: rate, dtype: float64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_cleaned.rate.value_counts()/len(file_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>be aware of things that seem inexplicable bec...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>the trump hotel collection is currently nomina...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>don t forget the open call at trump tower tomo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>looking forward to seeing the world champion y...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>to put on your calendar for may miss usa live ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  rate\n",
       "0   be aware of things that seem inexplicable bec...     2\n",
       "1  the trump hotel collection is currently nomina...     3\n",
       "2  don t forget the open call at trump tower tomo...     1\n",
       "3  looking forward to seeing the world champion y...     1\n",
       "4  to put on your calendar for may miss usa live ...    -1"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>great job on the larry king live gulf telethon...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>enter the contest http www facebook com sertam...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>the new president of opec is mahmoud ahmadinej...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>looking forward to the gop debate and the outc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>reporters say it s the trump bump i tell cnbc ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2282</td>\n",
       "      <td>for the first time in american history we have...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2292</td>\n",
       "      <td>elizabeth warren sometimes referred to as poca...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2298</td>\n",
       "      <td>john kerry and senator chris murphy grossly vi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>look so forward to being with my great friends...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2301</td>\n",
       "      <td>look forward to being with all of my friends a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>761 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  rate\n",
       "7     great job on the larry king live gulf telethon...     0\n",
       "12    enter the contest http www facebook com sertam...     0\n",
       "24    the new president of opec is mahmoud ahmadinej...     0\n",
       "26    looking forward to the gop debate and the outc...     0\n",
       "29    reporters say it s the trump bump i tell cnbc ...     0\n",
       "...                                                 ...   ...\n",
       "2282  for the first time in american history we have...     0\n",
       "2292  elizabeth warren sometimes referred to as poca...     0\n",
       "2298  john kerry and senator chris murphy grossly vi...     0\n",
       "2300  look so forward to being with my great friends...     0\n",
       "2301  look forward to being with all of my friends a...     0\n",
       "\n",
       "[761 rows x 2 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_cleaned[file_cleaned.rate==0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0    0.330582\n",
       "-1    0.188097\n",
       " 1    0.175934\n",
       " 2    0.086881\n",
       "-2    0.084275\n",
       "-3    0.046047\n",
       " 3    0.043440\n",
       "-4    0.016942\n",
       " 4    0.011729\n",
       "-5    0.004778\n",
       " 5    0.004344\n",
       "-6    0.003041\n",
       " 6    0.002172\n",
       "-7    0.000869\n",
       " 7    0.000434\n",
       "-8    0.000434\n",
       "Name: rate, dtype: float64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_cleaned.rate.value_counts()/len(file_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_word_list(text, remove_polish_letters):\n",
    "    ''' Pre process and convert texts to a list of words \n",
    "    method inspired by method from eliorc github repo: https://github.com/eliorc/Medium/blob/master/MaLSTM.ipynb'''\n",
    "    text = remove_polish_letters(text)\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Clean the text\n",
    "    text = sub(r\"[^A-Za-z0-9^,!?.\\/'+]\", \" \", text)\n",
    "    text = sub(r\"\\+\", \" plus \", text)\n",
    "    text = sub(r\",\", \" \", text)\n",
    "    text = sub(r\"\\.\", \" \", text)\n",
    "    text = sub(r\"!\", \" ! \", text)\n",
    "    text = sub(r\"\\?\", \" ? \", text)\n",
    "    text = sub(r\"'\", \" \", text)\n",
    "    text = sub(r\":\", \" : \", text)\n",
    "    text = sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "    text = text.split()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_cleaned.title = file_cleaned.title.apply(lambda x: text_to_word_list(x, unidecode))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_model = file_cleaned.copy()\n",
    "file_model = file_model[file_model.title.str.len()>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:46:43: collecting all words and their counts\n",
      "INFO - 16:46:43: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 16:46:43: collected 40838 word types from a corpus of 60132 words (unigram + bigrams) and 2302 sentences\n",
      "INFO - 16:46:43: using 40838 counts as vocab in Phrases<0 vocab, min_count=1, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 16:46:43: source_vocab length 40838\n",
      "INFO - 16:46:44: Phraser built with 3088 phrasegrams\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'trump_hotel',\n",
       " 'collection',\n",
       " 'is_currently',\n",
       " 'nominated_for',\n",
       " 'conde_nast',\n",
       " 'traveler_readers',\n",
       " 'choice_awards',\n",
       " 'travel_leisure',\n",
       " 'and',\n",
       " 'world',\n",
       " 'travel',\n",
       " 'awards']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = [row for row in file_model.title]\n",
    "phrases = Phrases(sent, min_count=1, progress_per=50000)\n",
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[sent]\n",
    "sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:46:44: collecting all words and their counts\n",
      "INFO - 16:46:44: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 16:46:44: collected 9541 word types from a corpus of 48723 raw words and 2302 sentences\n",
      "INFO - 16:46:44: Loading a fresh vocabulary\n",
      "INFO - 16:46:44: effective_min_count=3 retains 2861 unique words (29% of original 9541, drops 6680)\n",
      "INFO - 16:46:44: effective_min_count=3 leaves 39857 word corpus (81% of original 48723, drops 8866)\n",
      "INFO - 16:46:44: deleting the raw counts dictionary of 9541 items\n",
      "INFO - 16:46:44: sample=1e-05 downsamples 2861 most-common words\n",
      "INFO - 16:46:44: downsampling leaves estimated 6297 word corpus (15.8% of prior 39857)\n",
      "INFO - 16:46:44: estimated required memory for 2861 words and 300 dimensions: 8296900 bytes\n",
      "INFO - 16:46:44: resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.02 mins\n"
     ]
    }
   ],
   "source": [
    "\n",
    "w2v_model = Word2Vec(min_count=3,\n",
    "                     window=4,\n",
    "                     size=300,\n",
    "                     sample=1e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=multiprocessing.cpu_count()-1)\n",
    "\n",
    "start = time()\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per=50000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - start) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:46:45: training model with 3 workers on 2861 vocabulary and 300 features, using sg=0 hs=0 sample=1e-05 negative=20 window=4\n",
      "INFO - 16:46:46: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:46:46: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:46:46: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:46:46: EPOCH - 1 : training on 48723 raw words (6281 effective words) took 0.3s, 22044 effective words/s\n",
      "INFO - 16:46:46: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:46:46: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:46:46: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:46:46: EPOCH - 2 : training on 48723 raw words (6295 effective words) took 0.3s, 24727 effective words/s\n",
      "INFO - 16:46:46: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:46:46: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:46:46: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:46:46: EPOCH - 3 : training on 48723 raw words (6217 effective words) took 0.3s, 22209 effective words/s\n",
      "INFO - 16:46:47: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:46:47: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:46:47: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:46:47: EPOCH - 4 : training on 48723 raw words (6331 effective words) took 0.3s, 22258 effective words/s\n",
      "INFO - 16:46:47: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:46:47: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:46:47: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:46:47: EPOCH - 5 : training on 48723 raw words (6261 effective words) took 0.3s, 23706 effective words/s\n",
      "INFO - 16:46:47: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:46:47: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:46:47: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:46:47: EPOCH - 6 : training on 48723 raw words (6229 effective words) took 0.3s, 24387 effective words/s\n",
      "INFO - 16:46:47: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:46:47: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:46:47: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:46:47: EPOCH - 7 : training on 48723 raw words (6332 effective words) took 0.3s, 23623 effective words/s\n",
      "INFO - 16:46:48: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:46:48: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:46:48: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:46:48: EPOCH - 8 : training on 48723 raw words (6290 effective words) took 0.2s, 25272 effective words/s\n",
      "INFO - 16:46:48: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:46:48: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:46:48: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:46:48: EPOCH - 9 : training on 48723 raw words (6304 effective words) took 0.2s, 25563 effective words/s\n",
      "INFO - 16:46:48: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:46:48: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:46:48: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:46:48: EPOCH - 10 : training on 48723 raw words (6317 effective words) took 0.3s, 23221 effective words/s\n",
      "INFO - 16:46:48: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:46:48: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:46:48: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:46:48: EPOCH - 11 : training on 48723 raw words (6250 effective words) took 0.3s, 24359 effective words/s\n",
      "INFO - 16:46:49: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:46:49: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:46:49: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:46:49: EPOCH - 12 : training on 48723 raw words (6224 effective words) took 0.3s, 21571 effective words/s\n",
      "INFO - 16:46:49: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:46:49: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:46:49: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:46:49: EPOCH - 13 : training on 48723 raw words (6253 effective words) took 0.3s, 21712 effective words/s\n",
      "INFO - 16:46:49: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:46:49: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:46:49: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:46:49: EPOCH - 14 : training on 48723 raw words (6333 effective words) took 0.2s, 25572 effective words/s\n",
      "INFO - 16:46:50: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:46:50: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:46:50: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:46:50: EPOCH - 15 : training on 48723 raw words (6271 effective words) took 0.2s, 28873 effective words/s\n",
      "INFO - 16:46:50: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:46:50: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:46:50: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:46:50: EPOCH - 16 : training on 48723 raw words (6226 effective words) took 0.2s, 25909 effective words/s\n",
      "INFO - 16:46:50: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:46:50: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:46:50: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:46:50: EPOCH - 17 : training on 48723 raw words (6250 effective words) took 0.2s, 26691 effective words/s\n",
      "INFO - 16:46:50: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:46:50: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:46:50: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:46:50: EPOCH - 18 : training on 48723 raw words (6458 effective words) took 0.2s, 26829 effective words/s\n",
      "INFO - 16:46:51: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:46:51: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:46:51: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:46:51: EPOCH - 19 : training on 48723 raw words (6290 effective words) took 0.2s, 27702 effective words/s\n",
      "INFO - 16:46:51: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:46:51: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:46:51: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:46:51: EPOCH - 20 : training on 48723 raw words (6277 effective words) took 0.2s, 25758 effective words/s\n",
      "INFO - 16:46:51: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:46:51: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:46:51: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:46:51: EPOCH - 21 : training on 48723 raw words (6244 effective words) took 0.4s, 14892 effective words/s\n",
      "INFO - 16:46:52: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:46:52: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:46:52: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:46:52: EPOCH - 22 : training on 48723 raw words (6220 effective words) took 0.4s, 15826 effective words/s\n",
      "INFO - 16:46:52: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:46:52: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:46:52: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:46:52: EPOCH - 23 : training on 48723 raw words (6293 effective words) took 0.3s, 23650 effective words/s\n",
      "INFO - 16:46:52: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:46:52: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:46:52: worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:46:52: EPOCH - 24 : training on 48723 raw words (6314 effective words) took 0.3s, 20477 effective words/s\n",
      "INFO - 16:46:52: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:46:52: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:46:52: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:46:52: EPOCH - 25 : training on 48723 raw words (6333 effective words) took 0.2s, 26917 effective words/s\n",
      "INFO - 16:46:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:46:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:46:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:46:53: EPOCH - 26 : training on 48723 raw words (6249 effective words) took 0.2s, 29716 effective words/s\n",
      "INFO - 16:46:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:46:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:46:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:46:53: EPOCH - 27 : training on 48723 raw words (6353 effective words) took 0.2s, 29606 effective words/s\n",
      "INFO - 16:46:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:46:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:46:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:46:53: EPOCH - 28 : training on 48723 raw words (6255 effective words) took 0.2s, 29253 effective words/s\n",
      "INFO - 16:46:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:46:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:46:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:46:53: EPOCH - 29 : training on 48723 raw words (6281 effective words) took 0.2s, 26060 effective words/s\n",
      "INFO - 16:46:54: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:46:54: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:46:54: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:46:54: EPOCH - 30 : training on 48723 raw words (6294 effective words) took 0.2s, 26043 effective words/s\n",
      "INFO - 16:46:54: training on a 1461690 raw words (188525 effective words) took 8.2s, 22863 effective words/s\n",
      "INFO - 16:46:54: precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 0.14 mins\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - start) / 60, 2)))\n",
    "\n",
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:46:54: saving Word2Vec object under word2vec.model, separately None\n",
      "INFO - 16:46:54: not storing attribute vectors_norm\n",
      "INFO - 16:46:54: not storing attribute cum_table\n",
      "INFO - 16:46:54: saved word2vec.model\n"
     ]
    }
   ],
   "source": [
    "w2v_model.save(\"word2vec.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_export = file_model.copy()\n",
    "file_export['old_title'] = file_export.title\n",
    "file_export.old_title = file_export.old_title.str.join(' ')\n",
    "file_export.title = file_export.title.apply(lambda x: ' '.join(bigram[x]))\n",
    "file_export.rate = file_export.rate.astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_export[['title', 'rate']].to_csv('cleaned_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:46:54: loading Word2Vec object from /Users/pedram/Desktop/Trump-Tweets.nosync/word2vec.model\n",
      "INFO - 16:46:54: loading wv recursively from /Users/pedram/Desktop/Trump-Tweets.nosync/word2vec.model.wv.* with mmap=None\n",
      "INFO - 16:46:54: setting ignored attribute vectors_norm to None\n",
      "INFO - 16:46:54: loading vocabulary recursively from /Users/pedram/Desktop/Trump-Tweets.nosync/word2vec.model.vocabulary.* with mmap=None\n",
      "INFO - 16:46:54: loading trainables recursively from /Users/pedram/Desktop/Trump-Tweets.nosync/word2vec.model.trainables.* with mmap=None\n",
      "INFO - 16:46:54: setting ignored attribute cum_table to None\n",
      "INFO - 16:46:54: loaded /Users/pedram/Desktop/Trump-Tweets.nosync/word2vec.model\n"
     ]
    }
   ],
   "source": [
    "word_vectors = Word2Vec.load(\"/Users/pedram/Desktop/Trump-Tweets.nosync/word2vec.model\").wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = KMeans(n_clusters=2, max_iter=1000, random_state=True, n_init=50).fit(X=word_vectors.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09645423,  0.05619687,  0.01035173, ..., -0.06809834,\n",
       "        -0.03408125,  0.00757386],\n",
       "       [ 0.09693371,  0.05719624,  0.01004081, ..., -0.06882748,\n",
       "        -0.03310496,  0.0075336 ],\n",
       "       [ 0.09721133,  0.05690863,  0.01037155, ..., -0.06832389,\n",
       "        -0.0338853 ,  0.00711054],\n",
       "       ...,\n",
       "       [ 0.09652096,  0.05669889,  0.009251  , ..., -0.06920835,\n",
       "        -0.03335091,  0.00801022],\n",
       "       [ 0.09599321,  0.05769303,  0.00982091, ..., -0.06925657,\n",
       "        -0.0330694 ,  0.0075136 ],\n",
       "       [ 0.09594648,  0.05747993,  0.00957124, ..., -0.06794526,\n",
       "        -0.03331484,  0.00679584]], dtype=float32)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:47:02: precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('no', 0.9999821782112122),\n",
       " ('much', 0.9999821186065674),\n",
       " ('th', 0.9999819993972778),\n",
       " ('military', 0.9999816417694092),\n",
       " ('t_co', 0.9999814629554749),\n",
       " ('trump', 0.9999814629554749),\n",
       " ('report', 0.9999814033508301),\n",
       " ('gas', 0.9999812841415405),\n",
       " ('then', 0.999981164932251),\n",
       " ('never', 0.9999810457229614)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_vector(model.cluster_centers_[0], topn=10, restrict_vocab=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_cluster_center = model.cluster_centers_[0]\n",
    "negative_cluster_center = model.cluster_centers_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "words = pd.DataFrame(word_vectors.vocab.keys())\n",
    "words.columns = ['words']\n",
    "words['vectors'] = words.words.apply(lambda x: word_vectors.wv[f'{x}'])\n",
    "words['cluster'] = words.vectors.apply(lambda x: model.predict([np.array(x)]))\n",
    "words.cluster = words.cluster.apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "words['cluster_value'] = [1 if i==0 else -1 for i in words.cluster]\n",
    "words['closeness_score'] = words.apply(lambda x: 1/(model.transform([x.vectors]).min()), axis=1)\n",
    "words['sentiment_coeff'] = words.closeness_score * words.cluster_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>vectors</th>\n",
       "      <th>cluster</th>\n",
       "      <th>cluster_value</th>\n",
       "      <th>closeness_score</th>\n",
       "      <th>sentiment_coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>be</td>\n",
       "      <td>[0.09705158, 0.057498783, 0.0094628725, 0.0619...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>141.810578</td>\n",
       "      <td>141.810578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>aware_of</td>\n",
       "      <td>[0.09751691, 0.056494646, 0.009595476, 0.06193...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>118.275708</td>\n",
       "      <td>118.275708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>things</td>\n",
       "      <td>[0.096858695, 0.057343293, 0.009315238, 0.0630...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>139.340914</td>\n",
       "      <td>-139.340914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>that</td>\n",
       "      <td>[0.09611674, 0.05759553, 0.009847118, 0.062609...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>130.198072</td>\n",
       "      <td>-130.198072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>because_they</td>\n",
       "      <td>[0.09617047, 0.056552023, 0.010074812, 0.06336...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112.512841</td>\n",
       "      <td>112.512841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>can</td>\n",
       "      <td>[0.09736232, 0.05702704, 0.009217887, 0.061821...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>152.890420</td>\n",
       "      <td>152.890420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>a_big</td>\n",
       "      <td>[0.097173795, 0.057667114, 0.009345377, 0.0616...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>128.498328</td>\n",
       "      <td>-128.498328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>step_towards</td>\n",
       "      <td>[0.09647457, 0.05770414, 0.010114551, 0.062337...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>114.817276</td>\n",
       "      <td>-114.817276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>donald_j</td>\n",
       "      <td>[0.09615731, 0.056125242, 0.010574464, 0.06286...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>98.236321</td>\n",
       "      <td>98.236321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>trump</td>\n",
       "      <td>[0.09713665, 0.056823894, 0.009500767, 0.06293...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163.831315</td>\n",
       "      <td>163.831315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          words                                            vectors  cluster  \\\n",
       "0            be  [0.09705158, 0.057498783, 0.0094628725, 0.0619...        0   \n",
       "1      aware_of  [0.09751691, 0.056494646, 0.009595476, 0.06193...        0   \n",
       "2        things  [0.096858695, 0.057343293, 0.009315238, 0.0630...        1   \n",
       "3          that  [0.09611674, 0.05759553, 0.009847118, 0.062609...        1   \n",
       "4  because_they  [0.09617047, 0.056552023, 0.010074812, 0.06336...        0   \n",
       "5           can  [0.09736232, 0.05702704, 0.009217887, 0.061821...        0   \n",
       "6         a_big  [0.097173795, 0.057667114, 0.009345377, 0.0616...        1   \n",
       "7  step_towards  [0.09647457, 0.05770414, 0.010114551, 0.062337...        1   \n",
       "8      donald_j  [0.09615731, 0.056125242, 0.010574464, 0.06286...        0   \n",
       "9         trump  [0.09713665, 0.056823894, 0.009500767, 0.06293...        0   \n",
       "\n",
       "   cluster_value  closeness_score  sentiment_coeff  \n",
       "0              1       141.810578       141.810578  \n",
       "1              1       118.275708       118.275708  \n",
       "2             -1       139.340914      -139.340914  \n",
       "3             -1       130.198072      -130.198072  \n",
       "4              1       112.512841       112.512841  \n",
       "5              1       152.890420       152.890420  \n",
       "6             -1       128.498328      -128.498328  \n",
       "7             -1       114.817276      -114.817276  \n",
       "8              1        98.236321        98.236321  \n",
       "9              1       163.831315       163.831315  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[['words', 'sentiment_coeff']].to_csv('sentiment_dictionary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_file = pd.read_csv('/Users/pedram/Desktop/Trump-Tweets.nosync/cleaned_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_map = pd.read_csv('/Users/pedram/Desktop/Trump-Tweets.nosync/sentiment_dictionary.csv')\n",
    "sentiment_dict = dict(zip(sentiment_map.words.values, sentiment_map.sentiment_coeff.values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_weighting = final_file.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=lambda y: y.split(), norm=None)\n",
    "tfidf.fit(file_weighting.title)\n",
    "features = pd.Series(tfidf.get_feature_names())\n",
    "transformed = tfidf.transform(file_weighting.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfidf_dictionary(x, transformed_file, features):\n",
    "    '''\n",
    "    create dictionary for each input sentence x, where each word has assigned its tfidf score\n",
    "    \n",
    "    inspired  by function from this wonderful article: \n",
    "    https://medium.com/analytics-vidhya/automated-keyword-extraction-from-articles-using-nlp-bfd864f41b34\n",
    "    \n",
    "    x - row of dataframe, containing sentences, and their indexes,\n",
    "    transformed_file - all sentences transformed with TfidfVectorizer\n",
    "    features - names of all words in corpus used in TfidfVectorizer\n",
    "\n",
    "    '''\n",
    "    vector_coo = transformed_file[x.name].tocoo()\n",
    "    vector_coo.col = features.iloc[vector_coo.col].values\n",
    "    dict_from_coo = dict(zip(vector_coo.col, vector_coo.data))\n",
    "    return dict_from_coo\n",
    "\n",
    "def replace_tfidf_words(x, transformed_file, features):\n",
    "    '''\n",
    "    replacing each word with it's calculated tfidf dictionary with scores of each word\n",
    "    x - row of dataframe, containing sentences, and their indexes,\n",
    "    transformed_file - all sentences transformed with TfidfVectorizer\n",
    "    features - names of all words in corpus used in TfidfVectorizer\n",
    "    '''\n",
    "    dictionary = create_tfidf_dictionary(x, transformed_file, features)   \n",
    "    return list(map(lambda y:dictionary[f'{y}'], x.title.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.12 s, sys: 7.57 ms, total: 1.12 s\n",
      "Wall time: 1.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "replaced_tfidf_scores = file_weighting.apply(lambda x: replace_tfidf_words(x, transformed, features), axis=1)#this step takes around 3-4 minutes minutes to calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_sentiment_words(word, sentiment_dict):\n",
    "    '''\n",
    "    replacing each word with its associated sentiment score from sentiment dict\n",
    "    '''\n",
    "    try:\n",
    "        out = sentiment_dict[word]\n",
    "    except KeyError:\n",
    "        out = 0\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "replaced_closeness_scores = file_weighting.title.apply(lambda x: list(map(lambda y: replace_sentiment_words(y, sentiment_dict), x.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_df = pd.DataFrame(data=[replaced_closeness_scores, replaced_tfidf_scores, file_weighting.title, file_weighting.rate]).T\n",
    "replacement_df.columns = ['sentiment_coeff', 'tfidf_scores', 'sentence', 'sentiment']\n",
    "replacement_df['sentiment_rate'] = replacement_df.apply(lambda x: np.array(x.loc['sentiment_coeff']) @ np.array(x.loc['tfidf_scores']), axis=1)\n",
    "replacement_df['prediction'] = (replacement_df.sentiment_rate>0).astype('int8')\n",
    "replacement_df['sentiment'] = [1 if i==1 else 0 for i in replacement_df.sentiment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>831</td>\n",
       "      <td>1066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>190</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1\n",
       "0  831  1066\n",
       "1  190   215"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " Scores\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.454387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>precision</td>\n",
       "      <td>0.167838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>recall</td>\n",
       "      <td>0.530864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>f1</td>\n",
       "      <td>0.255042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             scores\n",
       "accuracy   0.454387\n",
       "precision  0.167838\n",
       "recall     0.530864\n",
       "f1         0.255042"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted_classes = replacement_df.prediction\n",
    "y_test = replacement_df.sentiment\n",
    "\n",
    "conf_matrix = pd.DataFrame(confusion_matrix(replacement_df.sentiment, replacement_df.prediction))\n",
    "print('Confusion Matrix')\n",
    "display(conf_matrix)\n",
    "\n",
    "test_scores = accuracy_score(y_test,predicted_classes), precision_score(y_test, predicted_classes), recall_score(y_test, predicted_classes), f1_score(y_test, predicted_classes)\n",
    "\n",
    "print('\\n \\n Scores')\n",
    "scores = pd.DataFrame(data=[test_scores])\n",
    "scores.columns = ['accuracy', 'precision', 'recall', 'f1']\n",
    "scores = scores.T\n",
    "scores.columns = ['scores']\n",
    "display(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
